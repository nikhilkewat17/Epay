FROM openjdk:11-jre-slim

ENV SPARK_VERSION=3.5.2
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/spark
ENV PATH="${PATH}:${SPARK_HOME}/bin"

# Install curl, tar, python3 and pip
RUN apt-get update && apt-get install -y curl tar python3 python3-pip && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Download and extract Apache Spark
RUN curl -L -o spark.tgz https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} $SPARK_HOME && \
    rm spark.tgz

# Copy your PySpark app script into container
COPY spark_pt.py /opt/spark/app/

WORKDIR $SPARK_HOME/work-dir

# Set entrypoint to spark-submit so you can run spark jobs easily
ENTRYPOINT ["/spark/bin/spark-submit", "/opt/spark/app/spark_pt.py"]
