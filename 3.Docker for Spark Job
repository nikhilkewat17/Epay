Great! Letâ€™s break it down step-by-step. Iâ€™ll give you:

1. âœ… All Docker files and job code structure


2. ğŸ“‚ Where and how to write YAML files for Spark jobs


3. ğŸ› ï¸ How to build and deploy everything on OpenShift




---

âœ… PART 1: Project Structure â€“ Spark + Docker + YAML

spark-openshift/
â”‚
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ txn_stream_job.py         # your PySpark code
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                # container image
â”‚   â””â”€â”€ entrypoint.sh             # entrypoint for spark-submit
â”‚
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ spark-job.yaml            # Job definition YAML
â”‚   â”œâ”€â”€ spark-service.yaml        # Optional: expose Spark UI
â”‚   â”œâ”€â”€ pvc.yaml                  # Volume
â”‚   â”œâ”€â”€ configmap.yaml            # Environment config
â”‚   â””â”€â”€ secret.yaml               # Secrets for DB/Kafka
â”‚
â”œâ”€â”€ deploy/
â”‚   â””â”€â”€ deploy.sh                 # Script to apply all YAMLs
â”‚
â””â”€â”€ README.md


---

ğŸ“ PART 2: All Docker Files & Spark Job

ğŸ“„ jobs/txn_stream_job.py

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("TxnStreamJob") \
    .getOrCreate()

# Simulated input source
data = [("txn001", 100), ("txn002", 500), ("txn003", 250)]
df = spark.createDataFrame(data, ["txn_id", "amount"])

df.show()

spark.stop()


---

ğŸ“„ docker/Dockerfile

FROM bitnami/spark:3.4.1

WORKDIR /opt/spark-app

COPY entrypoint.sh ./entrypoint.sh
COPY ../jobs ./jobs

RUN chmod +x ./entrypoint.sh

ENTRYPOINT ["./entrypoint.sh"]


---

ğŸ“„ docker/entrypoint.sh

#!/bin/bash

echo "Launching Spark Job..."

spark-submit \
  --master k8s://https://kubernetes.default.svc \
  --deploy-mode cluster \
  --name txn-stream-job \
  --conf spark.kubernetes.namespace=dev-spark \
  --conf spark.kubernetes.container.image=image-registry.openshift-image-registry.svc:5000/dev-spark/spark-job:latest \
  --conf spark.executor.instances=2 \
  ./jobs/txn_stream_job.py


---

ğŸ“¦ PART 3: Build & Push Docker Image to OpenShift

1. Log in to OpenShift internal registry:



oc whoami -t  # Get token

2. Build and push:



docker build -t spark-job:latest docker/
docker tag spark-job:latest image-registry.openshift-image-registry.svc:5000/dev-spark/spark-job:latest
docker push image-registry.openshift-image-registry.svc:5000/dev-spark/spark-job:latest


---

ğŸ“ PART 4: YAML Files (Write Once, Reuse for Jobs)

ğŸ“„ k8s/spark-job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: spark-job
  namespace: dev-spark
spec:
  template:
    spec:
      containers:
      - name: spark
        image: image-registry.openshift-image-registry.svc:5000/dev-spark/spark-job:latest
        volumeMounts:
        - name: output
          mountPath: /opt/output
        envFrom:
        - configMapRef:
            name: spark-config
        - secretRef:
            name: spark-secret
      restartPolicy: Never
      volumes:
      - name: output
        persistentVolumeClaim:
          claimName: spark-pvc


---

ğŸ“„ k8s/pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: spark-pvc
  namespace: dev-spark
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


---

ğŸ“„ k8s/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  namespace: dev-spark
data:
  APP_ENV: dev
  LOG_LEVEL: DEBUG


---

ğŸ“„ k8s/secret.yaml

apiVersion: v1
kind: Secret
metadata:
  name: spark-secret
  namespace: dev-spark
type: Opaque
data:
  db-password: bXlwYXNzCg==   # base64 for "mypwd"


---

ğŸ“„ k8s/spark-service.yaml (Optional: Spark UI)

apiVersion: v1
kind: Service
metadata:
  name: spark-ui
  namespace: dev-spark
spec:
  selector:
    app: spark
  ports:
    - protocol: TCP
      port: 4040
      targetPort: 4040
      nodePort: 30040
  type: NodePort


---

ğŸš€ PART 5: Deploy All Spark Jobs

ğŸ“„ deploy/deploy.sh

#!/bin/bash

oc project dev-spark

oc apply -f ../k8s/configmap.yaml
oc apply -f ../k8s/secret.yaml
oc apply -f ../k8s/pvc.yaml
oc apply -f ../k8s/spark-job.yaml
oc apply -f ../k8s/spark-service.yaml

Run:

bash deploy/deploy.sh


---

ğŸ§  HOW & WHERE TO WRITE YAML FILES

Job Type	Where to Write YAML

ğŸ”„ Batch Job	k8s/spark-job.yaml with kind: Job
ğŸ“¡ Streaming Job	Replace batch/v1 with Custom SparkApplication (optional)
ğŸ›  Multiple Jobs	Create new YAML: k8s/txn-job.yaml, k8s/recon-job.yaml, etc.
ğŸ¯ Namespaces	Use dev-spark, uat-spark, prod-spark as needed.